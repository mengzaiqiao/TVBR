{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Beta-recsys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Loaded training set statistics\n",
      "+---------+------------+------------+--------------+-----------------+-------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   | col_order   |\n",
      "|---------+------------+------------+--------------+-----------------+-------------|\n",
      "| count   | 3857794    | 3857794    | 3857794      | 3857794         | 3857794     |\n",
      "| nunique | 23093      | 14565      | 1            | 3857794         | 373719      |\n",
      "+---------+------------+------------+--------------+-----------------+-------------+\n",
      "valid_data_0 statistics\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   |\n",
      "|---------+------------+------------+--------------+-----------------|\n",
      "| count   | 3076168    | 3076168    | 3076168      | 3076168         |\n",
      "| nunique | 22475      | 14565      | 2            | 1               |\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "test_data_0 statistics\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   |\n",
      "|---------+------------+------------+--------------+-----------------|\n",
      "| count   | 3072601    | 3072601    | 3072601      | 3072601         |\n",
      "| nunique | 22434      | 14565      | 2            | 1               |\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "--------------------------------------------------------------------------------\n",
      "After intersection, testing set [0] statistics\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   |\n",
      "|---------+------------+------------+--------------+-----------------|\n",
      "| count   | 3072601    | 3072601    | 3072601      | 3072601         |\n",
      "| nunique | 22434      | 14565      | 2            | 1               |\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "After intersection, validation set [0] statistics\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   |\n",
      "|---------+------------+------------+--------------+-----------------|\n",
      "| count   | 3076168    | 3076168    | 3076168      | 3076168         |\n",
      "| nunique | 22475      | 14565      | 2            | 1               |\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "Filling alias table\n",
      "Filling alias table\n",
      "Execute [__init__] method costing 84367.53 ms\n",
      "Execute [__init__] method costing 0.00 ms\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from beta_rec.data.grocery_data import GroceryData\n",
    "from beta_rec.datasets.instacart import Instacart_25\n",
    "\n",
    "seed = 2021\n",
    "random.seed(seed)  # Fix random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "\n",
    "# make sure that you have already download the Instacart data from this link: https://www.kaggle.com/c/instacart-market-basket-analysis#\n",
    "# uncompressed them and put them in this folder: ../datasets/instacart_25/raw/*.csv\n",
    "\n",
    "\n",
    "dataset = Instacart_25(\n",
    "    min_u_c=20, min_i_c=30, min_o_c=10\n",
    ")  # Specifying the filtering conditions.\n",
    "\n",
    "# Split the data\n",
    "split_dataset = dataset.load_temporal_basket_split(test_rate=0.2, n_test=10)\n",
    "data = GroceryData(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"config_file\": \"../configs/tvbr_default.json\"}\n",
    "config[\"n_sample\"] = 1000000  # To reduce the test running time\n",
    "config[\"max_epoch\"] = 80\n",
    "config[\"emb_dim\"] = 64\n",
    "config[\"time_step\"] = 50\n",
    "config[\"batch_size\"] = 2048\n",
    "# config[\"tunable\"] = [\n",
    "#     {\"name\": \"lr\", \"type\": \"choice\", \"values\": [0.5, 0.05, 0.025, 0.001, 0.005]},\n",
    "# ]\n",
    "# config[\"tune\"] = True\n",
    "# the 'config_file' key is required, that is used load a default config.\n",
    "# Other keys can be specified to replace the default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model intialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search default config file in /home/zm324/anaconda3/envs/beta_rec/configs/tvbr_default.json\n",
      "Found default config file in /home/zm324/anaconda3/envs/beta_rec/configs/tvbr_default.json\n",
      "loading config file /home/zm324/anaconda3/envs/beta_rec/configs/tvbr_default.json\n",
      "--------------------------------------------------------------------------------\n",
      "Received parameters from command line (or default):\n",
      "+----+-----------------------+------------------------------------+\n",
      "|    | keys                  | values                             |\n",
      "|----+-----------------------+------------------------------------|\n",
      "|  0 | system:root_dir       | /home/zm324/workspace/beta-recsys/ |\n",
      "|  1 | model:n_sample        | 1000000                            |\n",
      "|  2 | model:max_epoch       | 80                                 |\n",
      "|  3 | model:emb_dim         | 64                                 |\n",
      "|  4 | model:time_step       | 20                                 |\n",
      "|  5 | model:batch_size      | 2048                               |\n",
      "|  6 | model:lr              | 0.001                              |\n",
      "|  7 | dataset:item_fea_type | random                             |\n",
      "|  8 | dataset:dataset       | instacart_25                       |\n",
      "+----+-----------------------+------------------------------------+\n",
      "--------------------------------------------------------------------------------\n",
      "logs will save in file: /home/zm324/workspace/beta-recsys/logs/TVBR_default_20211212_123804_murihu .stdout.log .stderr.log\n",
      "2021-12-12 12:38:04 [INFO]-\n",
      "Python version: 3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0\n",
      "2021-12-12 12:38:04 [INFO]-\n",
      "2021-12-12 12:38:04 [INFO]-Pytorch version: 1.7.1\n",
      "2021-12-12 12:38:04 [INFO]-The intermediate running statuses will be reported in folder: /home/zm324/workspace/beta-recsys/runs/TVBR_default_20211212_123804_murihu\n",
      "2021-12-12 12:38:04 [INFO]-Model checkpoint will save in file: /home/zm324/workspace/beta-recsys/checkpoints/TVBR_default_20211212_123804_murihu\n",
      "2021-12-12 12:38:04 [INFO]-Performance result will save in file: /home/zm324/workspace/beta-recsys/results/tvbr_result.csv\n",
      "2021-12-12 12:38:04 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:38:04 [INFO]-System configs\n",
      "2021-12-12 12:38:04 [INFO]-\n",
      "+----+----------------+-----------------------------------------------------------------------------------+\n",
      "|    | keys           | values                                                                            |\n",
      "|----+----------------+-----------------------------------------------------------------------------------|\n",
      "|  0 | root_dir       | /home/zm324/workspace/beta-recsys                                                 |\n",
      "|  1 | log_dir        | logs/                                                                             |\n",
      "|  2 | result_dir     | results/                                                                          |\n",
      "|  3 | process_dir    | /home/zm324/workspace/beta-recsys/processes/                                      |\n",
      "|  4 | checkpoint_dir | checkpoints/                                                                      |\n",
      "|  5 | dataset_dir    | datasets/                                                                         |\n",
      "|  6 | run_dir        | /home/zm324/workspace/beta-recsys/runs/TVBR_default_20211212_123804_murihu        |\n",
      "|  7 | tune_dir       | /home/zm324/workspace/beta-recsys/tune_results/                                   |\n",
      "|  8 | device         | gpu                                                                               |\n",
      "|  9 | seed           | 2020                                                                              |\n",
      "| 10 | metrics        | ['ndcg', 'precision', 'recall', 'map']                                            |\n",
      "| 11 | k              | [5, 10, 20]                                                                       |\n",
      "| 12 | valid_metric   | ndcg                                                                              |\n",
      "| 13 | valid_k        | 10                                                                                |\n",
      "| 14 | result_file    | /home/zm324/workspace/beta-recsys/results/tvbr_result.csv                         |\n",
      "| 15 | save_mode      | average                                                                           |\n",
      "| 16 | model_run_id   | TVBR_default_20211212_123804_murihu                                               |\n",
      "| 17 | log_file       | /home/zm324/workspace/beta-recsys/logs/TVBR_default_20211212_123804_murihu        |\n",
      "| 18 | model_save_dir | /home/zm324/workspace/beta-recsys/checkpoints/TVBR_default_20211212_123804_murihu |\n",
      "+----+----------------+-----------------------------------------------------------------------------------\n",
      "2021-12-12 12:38:04 [INFO]-\n",
      "2021-12-12 12:38:04 [INFO]---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 12:38:04 [ERROR]-Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2021-12-12 12:38:04 [ERROR]-wandb: Currently logged in as: mengzaiqiao (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mengzaiqiao/TVBR/runs/jfygioad\" target=\"_blank\">peachy-water-60</a></strong> to <a href=\"https://wandb.ai/mengzaiqiao/TVBR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-12 12:38:05 [INFO]-Get a gpu with the most available memory : 1\n",
      "2021-12-12 12:38:05 [INFO]-Initializing test engine ...\n",
      "2021-12-12 12:38:05 [INFO]-Get a gpu with the most available memory : 1\n",
      "2021-12-12 12:38:07 [INFO]-Setting device for torch_engine cuda:1\n",
      "2021-12-12 12:38:07 [INFO]-\n",
      "TVBR(\n",
      "  (user_emb): Embedding(23093, 64)\n",
      "  (item_emb): Embedding(14565, 64)\n",
      "  (time_embdding): Embedding(21, 21)\n",
      "  (user_mean): Embedding(23093, 64)\n",
      "  (user_std): Embedding(23093, 64)\n",
      "  (item_mean): Embedding(14565, 64)\n",
      "  (item_std): Embedding(14565, 64)\n",
      "  (time2mean_u): Sequential(\n",
      "    (0): Linear(in_features=597, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (time2std_u): Sequential(\n",
      "    (0): Linear(in_features=597, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (time2mean_i): Sequential(\n",
      "    (0): Linear(in_features=597, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (time2std_i): Sequential(\n",
      "    (0): Linear(in_features=597, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "\n",
      "2021-12-12 12:38:07 [INFO]-\n",
      "2021-12-12 12:38:08 [INFO]-Initialize Sampler!\n",
      "2021-12-12 12:38:08 [INFO]-preparing training triples ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 12:38:09 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2021-12-12 12:38:09 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2021-12-12 12:38:09 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2021-12-12 12:38:09 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2021-12-12 12:38:09 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 20/20 [01:37<00:00,  4.87s/it]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]Epoch 0 starts !\n",
      "2021-12-12 12:40:40 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:42:21 [INFO]-[Training Epoch 0], log_like_loss 9.425058364868164 kl_loss: -0.1472238451242447 alpha: 0.001 lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 12:42:21 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2021-12-12 12:42:21 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2021-12-12 12:42:21 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-12 12:42:21 [INFO]-Execute [train_an_epoch] method costing 101131.56 ms\n",
      "  1%|1         | 1/80 [01:41<2:13:09, 101.13s/it]Epoch 1 starts !\n",
      "2021-12-12 12:42:21 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:44:02 [INFO]-[Training Epoch 1], log_like_loss 8.85126781463623 kl_loss: -0.28690066933631897 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:44:02 [INFO]-Execute [train_an_epoch] method costing 101064.23 ms\n",
      "  2%|2         | 2/80 [03:22<2:11:27, 101.13s/it]Epoch 2 starts !\n",
      "2021-12-12 12:44:02 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:45:43 [INFO]-[Training Epoch 2], log_like_loss 8.735783576965332 kl_loss: -0.6038933992385864 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:45:43 [INFO]-Execute [train_an_epoch] method costing 101331.88 ms\n",
      "  4%|3         | 3/80 [05:03<2:09:52, 101.20s/it]Epoch 3 starts !\n",
      "2021-12-12 12:45:43 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:47:24 [INFO]-[Training Epoch 3], log_like_loss 8.637981414794922 kl_loss: -0.9454001188278198 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:47:25 [INFO]-Execute [train_an_epoch] method costing 101314.21 ms\n",
      "  5%|5         | 4/80 [06:44<2:08:15, 101.25s/it]Epoch 4 starts !\n",
      "2021-12-12 12:47:25 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:49:06 [INFO]-[Training Epoch 4], log_like_loss 8.547961235046387 kl_loss: -1.2727781534194946 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:49:06 [INFO]-Execute [train_an_epoch] method costing 101603.66 ms\n",
      "  6%|6         | 5/80 [08:26<2:06:42, 101.37s/it]Epoch 5 starts !\n",
      "2021-12-12 12:49:06 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:50:47 [INFO]-[Training Epoch 5], log_like_loss 8.473600387573242 kl_loss: -1.602271318435669 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:50:47 [INFO]-Execute [train_an_epoch] method costing 101060.90 ms\n",
      "  8%|7         | 6/80 [10:07<2:04:55, 101.30s/it]Epoch 6 starts !\n",
      "2021-12-12 12:50:47 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:52:28 [INFO]-[Training Epoch 6], log_like_loss 8.40455436706543 kl_loss: -2.014051675796509 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:52:28 [INFO]-Execute [train_an_epoch] method costing 100953.83 ms\n",
      "  9%|8         | 7/80 [11:48<2:03:08, 101.21s/it]Epoch 7 starts !\n",
      "2021-12-12 12:52:28 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:54:10 [INFO]-[Training Epoch 7], log_like_loss 8.35877513885498 kl_loss: -2.463667869567871 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:54:10 [INFO]-Execute [train_an_epoch] method costing 101763.04 ms\n",
      " 10%|#         | 8/80 [13:30<2:01:40, 101.39s/it]Epoch 8 starts !\n",
      "2021-12-12 12:54:10 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:55:52 [INFO]-[Training Epoch 8], log_like_loss 8.313328742980957 kl_loss: -2.889249563217163 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:55:52 [INFO]-Execute [train_an_epoch] method costing 101470.16 ms\n",
      " 11%|#1        | 9/80 [15:12<2:00:01, 101.43s/it]Epoch 9 starts !\n",
      "2021-12-12 12:55:52 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:57:33 [INFO]-[Training Epoch 9], log_like_loss 8.274654388427734 kl_loss: -4.1900763511657715 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:57:33 [INFO]-Execute [train_an_epoch] method costing 101505.49 ms\n",
      " 12%|#2        | 10/80 [16:53<1:58:22, 101.47s/it]Epoch 10 starts !\n",
      "2021-12-12 12:57:33 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 12:59:14 [INFO]-[Training Epoch 10], log_like_loss 8.250936508178711 kl_loss: -5.110273361206055 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 12:59:14 [INFO]-Execute [train_an_epoch] method costing 101013.44 ms\n",
      " 14%|#3        | 11/80 [18:34<1:56:33, 101.35s/it]Epoch 11 starts !\n",
      "2021-12-12 12:59:14 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:00:56 [INFO]-[Training Epoch 11], log_like_loss 8.219112396240234 kl_loss: -5.315648078918457 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:00:56 [INFO]-Execute [train_an_epoch] method costing 101316.00 ms\n",
      " 15%|#5        | 12/80 [20:16<1:54:52, 101.36s/it]Epoch 12 starts !\n",
      "2021-12-12 13:00:56 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:02:37 [INFO]-[Training Epoch 12], log_like_loss 8.193137168884277 kl_loss: -5.531316757202148 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:02:37 [INFO]-Execute [train_an_epoch] method costing 101483.29 ms\n",
      " 16%|#6        | 13/80 [21:57<1:53:14, 101.41s/it]Epoch 13 starts !\n",
      "2021-12-12 13:02:37 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:04:19 [INFO]-[Training Epoch 13], log_like_loss 8.170153617858887 kl_loss: -8.26551342010498 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:04:19 [INFO]-Execute [train_an_epoch] method costing 101431.90 ms\n",
      " 18%|#7        | 14/80 [23:39<1:51:34, 101.43s/it]Epoch 14 starts !\n",
      "2021-12-12 13:04:19 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:06:01 [INFO]-[Training Epoch 14], log_like_loss 8.153827667236328 kl_loss: -9.064234733581543 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:06:01 [INFO]-Execute [train_an_epoch] method costing 102688.59 ms\n",
      " 19%|#8        | 15/80 [25:21<1:50:18, 101.83s/it]Epoch 15 starts !\n",
      "2021-12-12 13:06:01 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:07:42 [INFO]-[Training Epoch 15], log_like_loss 8.124550819396973 kl_loss: -9.192118644714355 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:07:42 [INFO]-Execute [train_an_epoch] method costing 100891.84 ms\n",
      " 20%|##        | 16/80 [27:02<1:48:19, 101.56s/it]Epoch 16 starts !\n",
      "2021-12-12 13:07:42 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:09:24 [INFO]-[Training Epoch 16], log_like_loss 8.113119125366211 kl_loss: -9.310894966125488 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:09:24 [INFO]-Execute [train_an_epoch] method costing 101684.48 ms\n",
      " 21%|##1       | 17/80 [28:44<1:46:41, 101.61s/it]Epoch 17 starts !\n",
      "2021-12-12 13:09:24 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:11:05 [INFO]-[Training Epoch 17], log_like_loss 8.097108840942383 kl_loss: -9.431981086730957 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:11:05 [INFO]-Execute [train_an_epoch] method costing 101200.20 ms\n",
      " 22%|##2       | 18/80 [30:25<1:44:53, 101.51s/it]Epoch 18 starts !\n",
      "2021-12-12 13:11:05 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:12:47 [INFO]-[Training Epoch 18], log_like_loss 8.079239845275879 kl_loss: -9.497955322265625 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:12:47 [INFO]-Execute [train_an_epoch] method costing 101855.28 ms\n",
      " 24%|##3       | 19/80 [32:07<1:43:19, 101.63s/it]Epoch 19 starts !\n",
      "2021-12-12 13:12:47 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:14:29 [INFO]-[Training Epoch 19], log_like_loss 8.063287734985352 kl_loss: -9.626243591308594 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:14:29 [INFO]-Execute [train_an_epoch] method costing 101691.57 ms\n",
      " 25%|##5       | 20/80 [33:49<1:41:39, 101.66s/it]Epoch 20 starts !\n",
      "2021-12-12 13:14:29 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:16:11 [INFO]-[Training Epoch 20], log_like_loss 8.052334785461426 kl_loss: -9.731115341186523 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:16:11 [INFO]-Execute [train_an_epoch] method costing 101767.24 ms\n",
      " 26%|##6       | 21/80 [35:31<1:40:00, 101.71s/it]Epoch 21 starts !\n",
      "2021-12-12 13:16:11 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:17:52 [INFO]-[Training Epoch 21], log_like_loss 8.03809928894043 kl_loss: -9.827366828918457 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:17:52 [INFO]-Execute [train_an_epoch] method costing 101610.22 ms\n",
      " 28%|##7       | 22/80 [37:12<1:38:18, 101.69s/it]Epoch 22 starts !\n",
      "2021-12-12 13:17:52 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:19:34 [INFO]-[Training Epoch 22], log_like_loss 8.016471862792969 kl_loss: -9.886417388916016 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:19:34 [INFO]-Execute [train_an_epoch] method costing 101479.90 ms\n",
      " 29%|##8       | 23/80 [38:54<1:36:33, 101.65s/it]Epoch 23 starts !\n",
      "2021-12-12 13:19:34 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:21:15 [INFO]-[Training Epoch 23], log_like_loss 8.006750106811523 kl_loss: -9.95093059539795 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:21:15 [INFO]-Execute [train_an_epoch] method costing 101367.99 ms\n",
      " 30%|###       | 24/80 [40:35<1:34:48, 101.58s/it]Epoch 24 starts !\n",
      "2021-12-12 13:21:15 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:22:57 [INFO]-[Training Epoch 24], log_like_loss 7.997768402099609 kl_loss: -10.010322570800781 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:22:57 [INFO]-Execute [train_an_epoch] method costing 101566.07 ms\n",
      " 31%|###1      | 25/80 [42:17<1:33:07, 101.59s/it]Epoch 25 starts !\n",
      "2021-12-12 13:22:57 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:24:38 [INFO]-[Training Epoch 25], log_like_loss 7.992260932922363 kl_loss: -10.076034545898438 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:24:38 [INFO]-Execute [train_an_epoch] method costing 101299.80 ms\n",
      " 32%|###2      | 26/80 [43:58<1:31:22, 101.52s/it]Epoch 26 starts !\n",
      "2021-12-12 13:24:38 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:26:19 [INFO]-[Training Epoch 26], log_like_loss 7.982041358947754 kl_loss: -10.110904693603516 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:26:20 [INFO]-Execute [train_an_epoch] method costing 101136.67 ms\n",
      " 34%|###3      | 27/80 [45:40<1:29:35, 101.42s/it]Epoch 27 starts !\n",
      "2021-12-12 13:26:20 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:28:02 [INFO]-[Training Epoch 27], log_like_loss 7.970792770385742 kl_loss: -10.155691146850586 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:28:02 [INFO]-Execute [train_an_epoch] method costing 102013.65 ms\n",
      " 35%|###5      | 28/80 [47:22<1:28:04, 101.62s/it]Epoch 28 starts !\n",
      "2021-12-12 13:28:02 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:29:43 [INFO]-[Training Epoch 28], log_like_loss 7.948634147644043 kl_loss: -10.19736385345459 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:29:43 [INFO]-Execute [train_an_epoch] method costing 101595.79 ms\n",
      " 36%|###6      | 29/80 [49:03<1:26:22, 101.63s/it]Epoch 29 starts !\n",
      "2021-12-12 13:29:43 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:31:25 [INFO]-[Training Epoch 29], log_like_loss 7.9434356689453125 kl_loss: -10.234045028686523 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:31:25 [INFO]-Execute [train_an_epoch] method costing 101698.98 ms\n",
      " 38%|###7      | 30/80 [50:45<1:24:43, 101.68s/it]Epoch 30 starts !\n",
      "2021-12-12 13:31:25 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:33:07 [INFO]-[Training Epoch 30], log_like_loss 7.940074920654297 kl_loss: -10.256890296936035 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:33:07 [INFO]-Execute [train_an_epoch] method costing 101577.27 ms\n",
      " 39%|###8      | 31/80 [52:27<1:23:01, 101.67s/it]Epoch 31 starts !\n",
      "2021-12-12 13:33:07 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:34:49 [INFO]-[Training Epoch 31], log_like_loss 7.931519508361816 kl_loss: -10.284485816955566 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:34:49 [INFO]-Execute [train_an_epoch] method costing 102212.46 ms\n",
      " 40%|####      | 32/80 [54:09<1:21:28, 101.85s/it]Epoch 32 starts !\n",
      "2021-12-12 13:34:49 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:36:31 [INFO]-[Training Epoch 32], log_like_loss 7.9220991134643555 kl_loss: -10.30451774597168 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:36:31 [INFO]-Execute [train_an_epoch] method costing 101735.32 ms\n",
      " 41%|####1     | 33/80 [55:51<1:19:45, 101.83s/it]Epoch 33 starts !\n",
      "2021-12-12 13:36:31 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:38:13 [INFO]-[Training Epoch 33], log_like_loss 7.919952392578125 kl_loss: -10.312631607055664 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:38:13 [INFO]-Execute [train_an_epoch] method costing 102107.36 ms\n",
      " 42%|####2     | 34/80 [57:33<1:18:08, 101.93s/it]Epoch 34 starts !\n",
      "2021-12-12 13:38:13 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:39:55 [INFO]-[Training Epoch 34], log_like_loss 7.902946949005127 kl_loss: -10.34485912322998 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:39:55 [INFO]-Execute [train_an_epoch] method costing 102443.39 ms\n",
      " 44%|####3     | 35/80 [59:15<1:16:34, 102.10s/it]Epoch 35 starts !\n",
      "2021-12-12 13:39:55 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:41:37 [INFO]-[Training Epoch 35], log_like_loss 7.892681121826172 kl_loss: -10.370267868041992 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:41:37 [INFO]-Execute [train_an_epoch] method costing 101758.31 ms\n",
      " 45%|####5     | 36/80 [1:00:57<1:14:48, 102.01s/it]Epoch 36 starts !\n",
      "2021-12-12 13:41:37 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:43:19 [INFO]-[Training Epoch 36], log_like_loss 7.889044284820557 kl_loss: -10.392712593078613 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:43:19 [INFO]-Execute [train_an_epoch] method costing 101864.97 ms\n",
      " 46%|####6     | 37/80 [1:02:39<1:13:05, 101.98s/it]Epoch 37 starts !\n",
      "2021-12-12 13:43:19 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:45:01 [INFO]-[Training Epoch 37], log_like_loss 7.882443904876709 kl_loss: -10.388986587524414 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:45:01 [INFO]-Execute [train_an_epoch] method costing 102032.48 ms\n",
      " 48%|####7     | 38/80 [1:04:21<1:11:24, 102.02s/it]Epoch 38 starts !\n",
      "2021-12-12 13:45:01 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:46:43 [INFO]-[Training Epoch 38], log_like_loss 7.876651287078857 kl_loss: -10.410274505615234 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:46:43 [INFO]-Execute [train_an_epoch] method costing 101779.27 ms\n",
      " 49%|####8     | 39/80 [1:06:03<1:09:40, 101.96s/it]Epoch 39 starts !\n",
      "2021-12-12 13:46:43 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:48:25 [INFO]-[Training Epoch 39], log_like_loss 7.87441873550415 kl_loss: -10.437883377075195 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:48:25 [INFO]-Execute [train_an_epoch] method costing 101600.31 ms\n",
      " 50%|#####     | 40/80 [1:07:45<1:07:55, 101.88s/it]Epoch 40 starts !\n",
      "2021-12-12 13:48:25 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:50:07 [INFO]-[Training Epoch 40], log_like_loss 7.865975379943848 kl_loss: -10.441068649291992 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:50:07 [INFO]-Execute [train_an_epoch] method costing 102238.61 ms\n",
      " 51%|#####1    | 41/80 [1:09:31<1:07:09, 103.33s/it]Epoch 41 starts !\n",
      "2021-12-12 13:50:12 [INFO]---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 13:51:13 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2021-12-12 13:51:13 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2021-12-12 13:51:13 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2021-12-12 13:51:13 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-12 13:51:13 [INFO]-Current testEngine.best_valid_performance 0\n",
      "2021-12-12 13:51:13 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:51:13 [INFO]-performance on validation at epoch 40\n",
      "2021-12-12 13:51:13 [INFO]-\n",
      "+----+--------------+----------+\n",
      "|    | metrics      |   values |\n",
      "|----+--------------+----------|\n",
      "|  0 | ndcg@10      | 0.728473 |\n",
      "|  1 | precision@10 | 0.677895 |\n",
      "|  2 | recall@10    | 0.25455  |\n",
      "|  3 | map@10       | 0.211432 |\n",
      "+----+--------------+----------\n",
      "2021-12-12 13:51:13 [INFO]-\n",
      "2021-12-12 13:51:13 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:51:13 [INFO]-Execute [train_eval_worker] method costing 61881.19 ms\n",
      "2021-12-12 13:52:52 [INFO]-[Training Epoch 41], log_like_loss 7.853490352630615 kl_loss: -10.443995475769043 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:52:52 [INFO]-Execute [train_an_epoch] method costing 160194.95 ms\n",
      " 52%|#####2    | 42/80 [1:12:16<1:17:08, 121.79s/it]Epoch 42 starts !\n",
      "2021-12-12 13:52:56 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:54:00 [INFO]-Current testEngine.best_valid_performance 0.7284730840866777\n",
      "2021-12-12 13:54:00 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:54:00 [INFO]-performance on validation at epoch 41\n",
      "2021-12-12 13:54:00 [INFO]-\n",
      "+----+--------------+----------+\n",
      "|    | metrics      |   values |\n",
      "|----+--------------+----------|\n",
      "|  0 | ndcg@10      | 0.731421 |\n",
      "|  1 | precision@10 | 0.680917 |\n",
      "|  2 | recall@10    | 0.255851 |\n",
      "|  3 | map@10       | 0.212928 |\n",
      "+----+--------------+----------\n",
      "2021-12-12 13:54:00 [INFO]-\n",
      "2021-12-12 13:54:00 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:54:00 [INFO]-Execute [train_eval_worker] method costing 63477.89 ms\n",
      "2021-12-12 13:55:38 [INFO]-[Training Epoch 42], log_like_loss 7.8560662269592285 kl_loss: -10.445905685424805 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:55:38 [INFO]-Execute [train_an_epoch] method costing 161890.61 ms\n",
      " 54%|#####3    | 43/80 [1:15:03<1:23:22, 135.19s/it]Epoch 43 starts !\n",
      "2021-12-12 13:55:43 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:56:46 [INFO]-Current testEngine.best_valid_performance 0.7314209567346377\n",
      "2021-12-12 13:56:46 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:56:46 [INFO]-performance on validation at epoch 42\n",
      "2021-12-12 13:56:46 [INFO]-\n",
      "+----+--------------+----------+\n",
      "|    | metrics      |   values |\n",
      "|----+--------------+----------|\n",
      "|  0 | ndcg@10      | 0.73357  |\n",
      "|  1 | precision@10 | 0.682808 |\n",
      "|  2 | recall@10    | 0.256751 |\n",
      "|  3 | map@10       | 0.214192 |\n",
      "+----+--------------+----------\n",
      "2021-12-12 13:56:46 [INFO]-\n",
      "2021-12-12 13:56:46 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:56:46 [INFO]-Execute [train_eval_worker] method costing 62999.14 ms\n",
      "2021-12-12 13:58:25 [INFO]-[Training Epoch 43], log_like_loss 7.849167823791504 kl_loss: -10.457948684692383 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 13:58:26 [INFO]-Execute [train_an_epoch] method costing 161331.00 ms\n",
      " 55%|#####5    | 44/80 [1:17:50<1:26:52, 144.80s/it]Epoch 44 starts !\n",
      "2021-12-12 13:58:30 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:59:32 [INFO]-Current testEngine.best_valid_performance 0.7335701151909263\n",
      "2021-12-12 13:59:32 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:59:32 [INFO]-performance on validation at epoch 43\n",
      "2021-12-12 13:59:32 [INFO]-\n",
      "+----+--------------+----------+\n",
      "|    | metrics      |   values |\n",
      "|----+--------------+----------|\n",
      "|  0 | ndcg@10      | 0.735215 |\n",
      "|  1 | precision@10 | 0.684627 |\n",
      "|  2 | recall@10    | 0.257391 |\n",
      "|  3 | map@10       | 0.215049 |\n",
      "+----+--------------+----------\n",
      "2021-12-12 13:59:32 [INFO]-\n",
      "2021-12-12 13:59:32 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 13:59:32 [INFO]-Execute [train_eval_worker] method costing 61517.43 ms\n",
      "2021-12-12 14:01:11 [INFO]-[Training Epoch 44], log_like_loss 7.832588195800781 kl_loss: -10.466838836669922 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 14:01:11 [INFO]-Execute [train_an_epoch] method costing 159837.26 ms\n",
      " 56%|#####6    | 45/80 [1:20:35<1:28:02, 150.94s/it]Epoch 45 starts !\n",
      "2021-12-12 14:01:15 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 14:02:19 [INFO]-Current testEngine.best_valid_performance 0.7352147561762872\n",
      "2021-12-12 14:02:19 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 14:02:19 [INFO]-performance on validation at epoch 44\n",
      "2021-12-12 14:02:19 [INFO]-\n",
      "+----+--------------+----------+\n",
      "|    | metrics      |   values |\n",
      "|----+--------------+----------|\n",
      "|  0 | ndcg@10      | 0.737419 |\n",
      "|  1 | precision@10 | 0.686621 |\n",
      "|  2 | recall@10    | 0.258133 |\n",
      "|  3 | map@10       | 0.216031 |\n",
      "+----+--------------+----------\n",
      "2021-12-12 14:02:19 [INFO]-\n",
      "2021-12-12 14:02:19 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 14:02:19 [INFO]-Execute [train_eval_worker] method costing 63326.86 ms\n",
      "2021-12-12 14:03:58 [INFO]-[Training Epoch 45], log_like_loss 7.834765434265137 kl_loss: -10.464187622070312 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 14:03:58 [INFO]-Execute [train_an_epoch] method costing 163019.76 ms\n",
      " 57%|#####7    | 46/80 [1:23:23<1:28:21, 155.91s/it]Epoch 46 starts !\n",
      "2021-12-12 14:04:03 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 14:05:05 [INFO]-Current testEngine.best_valid_performance 0.737418773719222\n",
      "2021-12-12 14:05:05 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 14:05:05 [INFO]-performance on validation at epoch 45\n",
      "2021-12-12 14:05:05 [INFO]-\n",
      "+----+--------------+----------+\n",
      "|    | metrics      |   values |\n",
      "|----+--------------+----------|\n",
      "|  0 | ndcg@10      | 0.73899  |\n",
      "|  1 | precision@10 | 0.688156 |\n",
      "|  2 | recall@10    | 0.258964 |\n",
      "|  3 | map@10       | 0.216949 |\n",
      "+----+--------------+----------\n",
      "2021-12-12 14:05:05 [INFO]-\n",
      "2021-12-12 14:05:05 [INFO]---------------------------------------------------------------------------------\n",
      "2021-12-12 14:05:05 [INFO]-Execute [train_eval_worker] method costing 61664.55 ms\n",
      "2021-12-12 14:06:44 [INFO]-[Training Epoch 46], log_like_loss 7.828361511230469 kl_loss: -10.460314750671387 alpha: 0.001 lr: 0.001\n",
      "2021-12-12 14:06:44 [INFO]-Execute [train_an_epoch] method costing 161400.37 ms\n",
      " 59%|#####8    | 47/80 [1:26:09<1:27:24, 158.92s/it]Epoch 47 starts !\n",
      "2021-12-12 14:06:49 [INFO]---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from beta_rec.recommenders import TVBR\n",
    "\n",
    "for item_fea_type in [\n",
    "    \"random\",\n",
    "    \"cate\",\n",
    "    \"cate_word2vec\",\n",
    "    \"cate_bert\",\n",
    "    \"cate_one_hot\",\n",
    "    \"random_word2vec\",\n",
    "    \"random_bert\",\n",
    "#     \"random_one_hot\",\n",
    "#     \"random_bert_word2vec_one_hot\",\n",
    "#     \"random_cate_word2vec\",\n",
    "#     \"random_cate_bert\",\n",
    "#     \"random_cate_one_hot\",\n",
    "#     \"random_cate_bert_word2vec_one_hot\",\n",
    "]:\n",
    "    config[\"item_fea_type\"] = item_fea_type\n",
    "    lr = 0.001\n",
    "    time_step = 20\n",
    "    config[\"lr\"] = lr\n",
    "    config[\"time_step\"] = time_step\n",
    "    config[\"root_dir\"] = \"/home/zm324/workspace/beta-recsys/\"\n",
    "    config[\"dataset\"] = \"instacart_25\"\n",
    "    model = TVBR(config)\n",
    "    model.train(data)\n",
    "    model.test(data.test)\n",
    "    # @To be discussed\n",
    "# model.train(train_df)\n",
    "# Case 1, without validation, stop training by loss or max_epoch\n",
    "\n",
    "# model.train(train_df,valid_df[0])\n",
    "# Case 2, with validation, stop training by performance on validation set\n",
    "\n",
    "# model.train(train_df,valid_df[0],test_df[0])\n",
    "# Case 3, same as Case 2, but also evaluate performance for each epoch on test set.\n",
    "\n",
    "# Note that the best model will be save automatically, and record the model-save-dir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beta_rec",
   "language": "python",
   "name": "beta_rec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
