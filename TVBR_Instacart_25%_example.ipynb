{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Beta-recsys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Loaded training set statistics\n",
      "+---------+------------+------------+--------------+-----------------+-------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   | col_order   |\n",
      "|---------+------------+------------+--------------+-----------------+-------------|\n",
      "| count   | 3857794    | 3857794    | 3857794      | 3857794         | 3857794     |\n",
      "| nunique | 23093      | 14565      | 1            | 3857794         | 373719      |\n",
      "+---------+------------+------------+--------------+-----------------+-------------+\n",
      "valid_data_0 statistics\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   |\n",
      "|---------+------------+------------+--------------+-----------------|\n",
      "| count   | 3076168    | 3076168    | 3076168      | 3076168         |\n",
      "| nunique | 22475      | 14565      | 2            | 1               |\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "test_data_0 statistics\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   |\n",
      "|---------+------------+------------+--------------+-----------------|\n",
      "| count   | 3072601    | 3072601    | 3072601      | 3072601         |\n",
      "| nunique | 22434      | 14565      | 2            | 1               |\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "--------------------------------------------------------------------------------\n",
      "After intersection, testing set [0] statistics\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   |\n",
      "|---------+------------+------------+--------------+-----------------|\n",
      "| count   | 3072601    | 3072601    | 3072601      | 3072601         |\n",
      "| nunique | 22434      | 14565      | 2            | 1               |\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "After intersection, validation set [0] statistics\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "|         | col_user   | col_item   | col_rating   | col_timestamp   |\n",
      "|---------+------------+------------+--------------+-----------------|\n",
      "| count   | 3076168    | 3076168    | 3076168      | 3076168         |\n",
      "| nunique | 22475      | 14565      | 2            | 1               |\n",
      "+---------+------------+------------+--------------+-----------------+\n",
      "Filling alias table\n",
      "Filling alias table\n",
      "Execute [__init__] method costing 81100.69 ms\n",
      "Execute [__init__] method costing 0.00 ms\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from beta_rec.data.grocery_data import GroceryData\n",
    "from beta_rec.datasets.instacart import Instacart_25\n",
    "\n",
    "seed = 2021\n",
    "random.seed(seed)  # Fix random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "\n",
    "# make sure that you have already download the Instacart data from this link: https://www.kaggle.com/c/instacart-market-basket-analysis#\n",
    "# uncompressed them and put them in this folder: ../datasets/instacart_25/raw/*.csv\n",
    "\n",
    "\n",
    "dataset = Instacart_25(\n",
    "    min_u_c=20, min_i_c=30, min_o_c=10\n",
    ")  # Specifying the filtering conditions.\n",
    "\n",
    "# Split the data\n",
    "split_dataset = dataset.load_temporal_basket_split(test_rate=0.2, n_test=10)\n",
    "data = GroceryData(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search default config file in /home/zm324/anaconda3/envs/beta_rec/configs/tvbr_default.json\n",
      "Found default config file in /home/zm324/anaconda3/envs/beta_rec/configs/tvbr_default.json\n",
      "loading config file /home/zm324/anaconda3/envs/beta_rec/configs/tvbr_default.json\n",
      "--------------------------------------------------------------------------------\n",
      "Received parameters from command line (or default):\n",
      "+----+-----------------------+------------------------------------+\n",
      "|    | keys                  | values                             |\n",
      "|----+-----------------------+------------------------------------|\n",
      "|  0 | system:root_dir       | /home/zm324/workspace/beta-recsys/ |\n",
      "|  1 | model:lr              | 0.001                              |\n",
      "|  2 | model:time_step       | 50                                 |\n",
      "|  3 | model:n_sample        | 1000000                            |\n",
      "|  4 | model:max_epoch       | 80                                 |\n",
      "|  5 | model:emb_dim         | 64                                 |\n",
      "|  6 | model:batch_size      | 1024                               |\n",
      "|  7 | dataset:item_fea_type | random_word2vec                    |\n",
      "|  8 | dataset:dataset       | instacart_25                       |\n",
      "+----+-----------------------+------------------------------------+\n",
      "--------------------------------------------------------------------------------\n",
      "logs will save in file: /home/zm324/workspace/beta-recsys/logs/TVBR_default_20220116_120914_murihu .stdout.log .stderr.log\n",
      "2022-01-16 12:09:15 [INFO]-\n",
      "Python version: 3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0\n",
      "2022-01-16 12:09:15 [INFO]-\n",
      "2022-01-16 12:09:15 [INFO]-Pytorch version: 1.7.1\n",
      "2022-01-16 12:09:15 [INFO]-The intermediate running statuses will be reported in folder: /home/zm324/workspace/beta-recsys/runs/TVBR_default_20220116_120914_murihu\n",
      "2022-01-16 12:09:15 [INFO]-Model checkpoint will save in file: /home/zm324/workspace/beta-recsys/checkpoints/TVBR_default_20220116_120914_murihu\n",
      "2022-01-16 12:09:15 [INFO]-Performance result will save in file: /home/zm324/workspace/beta-recsys/results/tvbr_result.csv\n",
      "2022-01-16 12:09:15 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:09:15 [INFO]-System configs\n",
      "2022-01-16 12:09:15 [INFO]-\n",
      "+----+----------------+-----------------------------------------------------------------------------------+\n",
      "|    | keys           | values                                                                            |\n",
      "|----+----------------+-----------------------------------------------------------------------------------|\n",
      "|  0 | root_dir       | /home/zm324/workspace/beta-recsys                                                 |\n",
      "|  1 | log_dir        | logs/                                                                             |\n",
      "|  2 | result_dir     | results/                                                                          |\n",
      "|  3 | process_dir    | /home/zm324/workspace/beta-recsys/processes/                                      |\n",
      "|  4 | checkpoint_dir | checkpoints/                                                                      |\n",
      "|  5 | dataset_dir    | datasets/                                                                         |\n",
      "|  6 | run_dir        | /home/zm324/workspace/beta-recsys/runs/TVBR_default_20220116_120914_murihu        |\n",
      "|  7 | tune_dir       | /home/zm324/workspace/beta-recsys/tune_results/                                   |\n",
      "|  8 | device         | gpu                                                                               |\n",
      "|  9 | seed           | 2020                                                                              |\n",
      "| 10 | metrics        | ['ndcg', 'precision', 'recall', 'map']                                            |\n",
      "| 11 | k              | [5, 10, 20]                                                                       |\n",
      "| 12 | valid_metric   | ndcg                                                                              |\n",
      "| 13 | valid_k        | 10                                                                                |\n",
      "| 14 | result_file    | /home/zm324/workspace/beta-recsys/results/tvbr_result.csv                         |\n",
      "| 15 | save_mode      | average                                                                           |\n",
      "| 16 | model_run_id   | TVBR_default_20220116_120914_murihu                                               |\n",
      "| 17 | log_file       | /home/zm324/workspace/beta-recsys/logs/TVBR_default_20220116_120914_murihu        |\n",
      "| 18 | model_save_dir | /home/zm324/workspace/beta-recsys/checkpoints/TVBR_default_20220116_120914_murihu |\n",
      "+----+----------------+-----------------------------------------------------------------------------------\n",
      "2022-01-16 12:09:15 [INFO]-\n",
      "2022-01-16 12:09:15 [INFO]---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 12:09:15 [ERROR]-Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2022-01-16 12:09:15 [ERROR]-wandb: Currently logged in as: mengzaiqiao (use `wandb login --relogin` to force relogin)\n",
      "2022-01-16 12:09:16 [ERROR]-\n",
      "wandb: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mengzaiqiao/TVBR/runs/17f9glre\" target=\"_blank\">cool-microwave-81</a></strong> to <a href=\"https://wandb.ai/mengzaiqiao/TVBR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-16 12:09:16 [INFO]-0\n",
      "2022-01-16 12:09:16 [INFO]-Get a gpu with the most available memory : 0\n",
      "2022-01-16 12:09:16 [INFO]-Initializing test engine ...\n",
      "2022-01-16 12:09:16 [INFO]-0\n",
      "2022-01-16 12:09:16 [INFO]-Get a gpu with the most available memory : 0\n",
      "2022-01-16 12:09:16 [INFO]-load basic item featrue for dataset: instacart_25  type: word2vec\n",
      "2022-01-16 12:09:26 [INFO]-Setting device for torch_engine cuda:0\n",
      "2022-01-16 12:09:26 [INFO]-\n",
      "TVBR(\n",
      "  (user_emb): Embedding(23093, 64)\n",
      "  (item_emb): Embedding(14565, 64)\n",
      "  (time_embdding): Embedding(51, 51)\n",
      "  (user_mean): Embedding(23093, 64)\n",
      "  (user_std): Embedding(23093, 64)\n",
      "  (item_mean): Embedding(14565, 64)\n",
      "  (item_std): Embedding(14565, 64)\n",
      "  (time2mean_u): Sequential(\n",
      "    (0): Linear(in_features=627, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (time2std_u): Sequential(\n",
      "    (0): Linear(in_features=627, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (time2mean_i): Sequential(\n",
      "    (0): Linear(in_features=927, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (time2std_i): Sequential(\n",
      "    (0): Linear(in_features=927, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "\n",
      "2022-01-16 12:09:26 [INFO]-\n",
      "2022-01-16 12:09:27 [INFO]-Initialize Sampler!\n",
      "2022-01-16 12:09:27 [INFO]-preparing training triples ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 12:09:28 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2022-01-16 12:09:28 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2022-01-16 12:09:29 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2022-01-16 12:09:29 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2022-01-16 12:09:29 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 50/50 [01:36<00:00,  1.94s/it]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]Epoch 0 starts !\n",
      "2022-01-16 12:11:57 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:13:55 [INFO]-[Training Epoch 0], log_like_loss 36.711395263671875 kl_loss: -0.23360738158226013 alpha: 0.001 lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 12:13:55 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2022-01-16 12:13:55 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "2022-01-16 12:13:55 [ERROR]-wandb: WARNING When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-16 12:13:55 [INFO]-Execute [train_an_epoch] method costing 117814.84 ms\n",
      "  1%|1         | 1/80 [01:57<2:35:09, 117.84s/it]Epoch 1 starts !\n",
      "2022-01-16 12:13:55 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:15:54 [INFO]-[Training Epoch 1], log_like_loss 35.45085525512695 kl_loss: -0.15418459475040436 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:15:54 [INFO]-Execute [train_an_epoch] method costing 118618.46 ms\n",
      "  2%|2         | 2/80 [03:56<2:33:30, 118.09s/it]Epoch 2 starts !\n",
      "2022-01-16 12:15:54 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:17:53 [INFO]-[Training Epoch 2], log_like_loss 35.03792953491211 kl_loss: -0.16458794474601746 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:17:53 [INFO]-Execute [train_an_epoch] method costing 119089.99 ms\n",
      "  4%|3         | 3/80 [05:55<2:31:57, 118.40s/it]Epoch 3 starts !\n",
      "2022-01-16 12:17:53 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:19:52 [INFO]-[Training Epoch 3], log_like_loss 34.6660270690918 kl_loss: -0.21339698135852814 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:19:52 [INFO]-Execute [train_an_epoch] method costing 119394.54 ms\n",
      "  5%|5         | 4/80 [07:55<2:30:22, 118.72s/it]Epoch 4 starts !\n",
      "2022-01-16 12:19:52 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:21:52 [INFO]-[Training Epoch 4], log_like_loss 34.320804595947266 kl_loss: -1.3876360654830933 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:21:52 [INFO]-Execute [train_an_epoch] method costing 119642.61 ms\n",
      "  6%|6         | 5/80 [09:54<2:28:46, 119.02s/it]Epoch 5 starts !\n",
      "2022-01-16 12:21:52 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:23:52 [INFO]-[Training Epoch 5], log_like_loss 34.05827331542969 kl_loss: -3.094712018966675 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:23:52 [INFO]-Execute [train_an_epoch] method costing 120003.94 ms\n",
      "  8%|7         | 6/80 [11:54<2:27:10, 119.33s/it]Epoch 6 starts !\n",
      "2022-01-16 12:23:52 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:25:56 [INFO]-[Training Epoch 6], log_like_loss 33.82658386230469 kl_loss: -3.607510566711426 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:25:56 [INFO]-Execute [train_an_epoch] method costing 123915.77 ms\n",
      "  9%|8         | 7/80 [13:58<2:26:52, 120.73s/it]Epoch 7 starts !\n",
      "2022-01-16 12:25:56 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:27:56 [INFO]-[Training Epoch 7], log_like_loss 33.65329360961914 kl_loss: -4.244717121124268 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:27:56 [INFO]-Execute [train_an_epoch] method costing 119633.58 ms\n",
      " 10%|#         | 8/80 [15:58<2:24:29, 120.42s/it]Epoch 8 starts !\n",
      "2022-01-16 12:27:56 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:29:58 [INFO]-[Training Epoch 8], log_like_loss 33.482200622558594 kl_loss: -4.916393756866455 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:29:58 [INFO]-Execute [train_an_epoch] method costing 122384.00 ms\n",
      " 11%|#1        | 9/80 [18:00<2:23:12, 121.02s/it]Epoch 9 starts !\n",
      "2022-01-16 12:29:58 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:31:59 [INFO]-[Training Epoch 9], log_like_loss 33.32392883300781 kl_loss: -5.600983142852783 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:31:59 [INFO]-Execute [train_an_epoch] method costing 120423.68 ms\n",
      " 12%|#2        | 10/80 [20:01<2:21:00, 120.87s/it]Epoch 10 starts !\n",
      "2022-01-16 12:31:59 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:33:58 [INFO]-[Training Epoch 10], log_like_loss 33.19564437866211 kl_loss: -9.707880020141602 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:33:58 [INFO]-Execute [train_an_epoch] method costing 119339.62 ms\n",
      " 14%|#3        | 11/80 [22:00<2:18:29, 120.43s/it]Epoch 11 starts !\n",
      "2022-01-16 12:33:58 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:35:57 [INFO]-[Training Epoch 11], log_like_loss 33.10123825073242 kl_loss: -11.501039505004883 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:35:58 [INFO]-Execute [train_an_epoch] method costing 119172.07 ms\n",
      " 15%|#5        | 12/80 [24:00<2:16:05, 120.08s/it]Epoch 12 starts !\n",
      "2022-01-16 12:35:58 [INFO]---------------------------------------------------------------------------------\n",
      "2022-01-16 12:37:58 [INFO]-[Training Epoch 12], log_like_loss 32.96574783325195 kl_loss: -11.787766456604004 alpha: 0.001 lr: 0.001\n",
      "2022-01-16 12:37:58 [INFO]-Execute [train_an_epoch] method costing 120376.55 ms\n",
      " 16%|#6        | 13/80 [26:00<2:14:12, 120.19s/it]Epoch 13 starts !\n",
      "2022-01-16 12:37:58 [INFO]---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from beta_rec.recommenders import TVBR\n",
    "\n",
    "lr = 0.001\n",
    "time_step = 50\n",
    "config = {\"config_file\": \"../configs/tvbr_default.json\"}\n",
    "config[\"lr\"] = lr\n",
    "config[\"time_step\"] = time_step\n",
    "config[\"n_sample\"] = 1000000  # To reduce the test running time\n",
    "config[\"max_epoch\"] = 80\n",
    "config[\"emb_dim\"] = 64\n",
    "config[\"batch_size\"] = 1024\n",
    "config[\"item_fea_type\"] = \"random_word2vec\"\n",
    "config[\"root_dir\"] = \"/home/zm324/workspace/beta-recsys/\"\n",
    "config[\"dataset\"] = \"instacart_25\"\n",
    "model = TVBR(config)\n",
    "model.train(data)\n",
    "model.test(data.test[0])\n",
    "# @To be discussed\n",
    "# model.train(train_df)\n",
    "# Case 1, without validation, stop training by loss or max_epoch\n",
    "\n",
    "# model.train(train_df,valid_df[0])\n",
    "# Case 2, with validation, stop training by performance on validation set\n",
    "\n",
    "# model.train(train_df,valid_df[0],test_df[0])\n",
    "# Case 3, same as Case 2, but also evaluate performance for each epoch on test set.\n",
    "\n",
    "# Note that the best model will be save automatically, and record the model-save-dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beta_rec",
   "language": "python",
   "name": "beta_rec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
